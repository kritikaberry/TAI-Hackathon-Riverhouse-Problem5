{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_title",
   "metadata": {},
   "source": [
    "# ğŸ¤– AI Incident Database â€” Master Dataset Builder\n",
    "### Reproducible pipeline: CSET + GMF + MIT + Incidents â†’ cleaned, joined master\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘‹ How to use this notebook\n",
    "\n",
    "**For most users â€” just run everything:**\n",
    "1. Open this notebook in [Google Colab](https://colab.research.google.com)\n",
    "2. Click **Runtime â†’ Run all** (or press `Ctrl+F9`)\n",
    "3. Wait ~2 minutes â€” the master dataset will auto-download to your computer\n",
    "\n",
    "**If a source file's column has been renamed or a new column was added:**\n",
    "- Run **Cell 1 (âš™ï¸ Configuration)** and **Cell 3 (ğŸ“¥ Download)** first\n",
    "- Then run **Cell 4 (ğŸ” Schema Health Check)** â€” it will show you exactly which column names have changed\n",
    "- Update the relevant mapping in **Cell 1** and re-run from Cell 4 onwards\n",
    "\n",
    "---\n",
    "| Cell | What it does | Edit needed? |\n",
    "|------|-------------|-------------|\n",
    "| **1 Â· âš™ï¸ Configuration** | All settings & column mappings in one place | âœï¸ Only cell you ever edit |\n",
    "| **2 Â· ğŸ“¦ Install & Import** | Installs libraries, nothing to change | âŒ Never |\n",
    "| **3 Â· ğŸ“¥ Download Snapshot** | Downloads latest AIID data automatically | âŒ Never |\n",
    "| **4 Â· ğŸ” Schema Health Check** | Validates column names â€” flags any renames | âŒ Never |\n",
    "| **5 Â· ğŸ“‚ Load Raw Data** | Reads all CSVs into memory | âŒ Never |\n",
    "| **6â€“9 Â· ğŸ§¹ Clean each source** | Standardises, drops noise columns | âŒ Never |\n",
    "| **10 Â· ğŸ”— Build Master** | Joins all sources into one table | âŒ Never |\n",
    "| **11 Â· âœ… Validation** | Automated quality checks | âŒ Never |\n",
    "| **12 Â· ğŸ’¾ Export** | Saves formatted Excel + triggers download | âŒ Never |\n",
    "| **13 Â· ğŸ“Š Summary** | Final statistics | âŒ Never |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  âš™ï¸  CELL 1 â€” CONFIGURATION                                                 â•‘\n",
    "# â•‘  This is the ONLY cell you ever need to edit.                               â•‘\n",
    "# â•‘  Everything below is automatically used by the rest of the notebook.        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ 1A. FILE PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Where to save the downloaded snapshot and final output.\n",
    "# In Google Colab, files are saved under /content/ by default.\n",
    "\n",
    "SNAPSHOT_DIR = Path(\"/content/aiid_snapshot\")   # â† where downloaded files live\n",
    "OUTPUT_PATH  = Path(\"/content/AIID_Master_Dataset.xlsx\")  # â† final Excel file\n",
    "\n",
    "# â”€â”€ 1B. DOWNLOAD SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Leave these as-is unless the AIID website structure changes.\n",
    "\n",
    "BASE_URL          = \"https://incidentdatabase.ai\"\n",
    "SNAPSHOT_PAGE_URL = f\"{BASE_URL}/research/snapshots/\"\n",
    "SNAPSHOT_FILTER   = \".tar.bz2\"\n",
    "\n",
    "# â”€â”€ 1C. COLUMN MAPPINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Format:  \"Original column name in source file\" : \"Name to use in master dataset\"\n",
    "#\n",
    "# HOW TO UPDATE: If the Schema Health Check (Cell 4) flags a column as missing,\n",
    "# find that column in the dict below and update the KEY (left side) to match\n",
    "# the new column name shown in the health check output.\n",
    "# The VALUE (right side = master column name) should stay the same.\n",
    "\n",
    "# --- incidents.csv ---\n",
    "INCIDENTS_COLUMNS = {\n",
    "    # original name in file          : master column name\n",
    "    \"incident_id\"                    : \"Incident ID\",\n",
    "    \"date\"                           : \"date\",\n",
    "    \"title\"                          : \"title\",\n",
    "    \"description\"                    : \"description\",\n",
    "    \"reports\"                        : \"reports\",        # used to count reports, then dropped\n",
    "    \"Alleged deployer of AI system\"  : \"deployer_raw\",   # parsed to readable text\n",
    "    \"Alleged developer of AI system\" : \"developer_raw\",\n",
    "    \"Alleged harmed or nearly harmed parties\" : \"harmed_raw\",\n",
    "}\n",
    "\n",
    "# --- classifications_MIT.csv ---\n",
    "MIT_COLUMNS = {\n",
    "    \"Incident ID\"    : \"Incident ID\",\n",
    "    \"Risk Domain\"    : \"Risk Domain\",\n",
    "    \"Risk Subdomain\" : \"Risk Subdomain\",\n",
    "    \"Entity\"         : \"Responsible Entity\",\n",
    "    \"Timing\"         : \"Timing\",\n",
    "    \"Intent\"         : \"Intent\",\n",
    "}\n",
    "\n",
    "# --- classifications_GMF.csv ---\n",
    "GMF_COLUMNS = {\n",
    "    \"Incident ID\"                : \"Incident ID\",\n",
    "    \"Known AI Goal\"              : \"AI Goal\",\n",
    "    \"Known AI Technology\"        : \"AI Technology\",\n",
    "    \"Known AI Technical Failure\" : \"Technical Failure\",\n",
    "}\n",
    "\n",
    "# --- classifications_CSETv1.csv ---\n",
    "CSET_COLUMNS = {\n",
    "    \"Incident ID\"                      : \"Incident ID\",\n",
    "    \"Harm Domain\"                      : \"Harm Domain\",\n",
    "    \"Tangible Harm\"                    : \"Tangible Harm\",\n",
    "    \"AI Harm Level\"                    : \"AI Harm Level\",\n",
    "    \"Rights Violation\"                 : \"Rights Violation\",\n",
    "    \"Harm Distribution Basis\"          : \"Harm Distribution Basis\",\n",
    "    \"Special Interest Intangible Harm\" : \"Special Interest Intangible Harm\",\n",
    "    \"Lives Lost\"                       : \"Lives Lost\",\n",
    "    \"Injuries\"                         : \"Injuries\",\n",
    "    \"Estimated Harm Quantities\"        : \"Estimated Harm Quantities\",\n",
    "    \"Sector of Deployment\"             : \"Sector of Deployment\",\n",
    "    \"Public Sector Deployment\"         : \"Public Sector Deployment\",\n",
    "    \"Infrastructure Sectors\"           : \"Infrastructure Sectors\",\n",
    "    \"Location Region\"                  : \"Location Region\",\n",
    "    \"Location Country (two letters)\"   : \"Country Code\",\n",
    "    \"Location City\"                    : \"Location City\",\n",
    "    \"Date of Incident Year\"            : \"Incident Year\",\n",
    "    \"Date of Incident Month\"           : \"Incident Month\",\n",
    "    \"Date of Incident Day\"             : \"Incident Day\",\n",
    "    \"Intentional Harm\"                 : \"Intentional Harm\",\n",
    "    \"Autonomy Level\"                   : \"Autonomy Level\",\n",
    "    \"AI Task\"                          : \"AI Task\",\n",
    "    \"AI tools and methods\"             : \"AI Methods\",\n",
    "    \"AI System Description\"            : \"AI System Description\",\n",
    "    \"Data Inputs\"                      : \"Data Inputs\",\n",
    "    \"Physical Objects\"                 : \"Physical Objects\",\n",
    "    \"Deployed\"                         : \"Deployed\",\n",
    "    \"Involving Minor\"                  : \"Involving Minor\",\n",
    "    \"Protected Characteristic\"         : \"Protected Characteristic\",\n",
    "    \"AI System\"                        : \"AI System\",\n",
    "    \"Detrimental Content\"              : \"Detrimental Content\",\n",
    "    \"Impact on Critical Services\"      : \"Impact on Critical Services\",\n",
    "    \"Multiple AI Interaction\"          : \"Multiple AI Interaction\",\n",
    "    \"Embedded\"                         : \"Embedded\",\n",
    "    \"Entities\"                         : \"Entities\",\n",
    "    \"Estimated Date\"                   : \"Estimated Date\",\n",
    "}\n",
    "\n",
    "# --- duplicates.csv ---\n",
    "DUPLICATES_ID_COLUMN = \"duplicate_incident_number\"   # column holding IDs to remove\n",
    "\n",
    "# â”€â”€ 1D. VALIDATION THRESHOLDS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Adjust these if the database grows significantly.\n",
    "\n",
    "EXPECTED_MIN_INCIDENTS = 1300    # minimum expected row count after deduplication\n",
    "EXPECTED_MIT_COVERAGE  = 85.0    # minimum % of incidents with MIT Risk Domain\n",
    "\n",
    "# â”€â”€ 1E. MASTER COLUMN ORDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Controls column order in the output Excel file.\n",
    "# Columns not listed here are appended at the end automatically.\n",
    "\n",
    "MASTER_COLUMN_ORDER = [\n",
    "    # Core identity (100% filled â€” always use these for analysis)\n",
    "    \"Incident ID\", \"date\", \"year\", \"title\", \"description\",\n",
    "    \"deployer\", \"developer\", \"harmed\",\n",
    "    # Coverage flag\n",
    "    \"Data Sources\", \"report_count\",\n",
    "    # MIT taxonomy (91% coverage)\n",
    "    \"Risk Domain\", \"Risk Subdomain\", \"Responsible Entity\", \"Intent\", \"Timing\",\n",
    "    # GMF taxonomy (24% coverage)\n",
    "    \"AI Goal\", \"AI Technology\", \"Technical Failure\",\n",
    "    # CSET taxonomy (16% coverage)\n",
    "    \"Harm Domain\", \"Tangible Harm\", \"AI Harm Level\",\n",
    "    \"Rights Violation\", \"Harm Distribution Basis\",\n",
    "    \"Lives Lost\", \"Injuries\",\n",
    "    \"Sector of Deployment\", \"Public Sector Deployment\", \"Infrastructure Sectors\",\n",
    "    \"Location Region\", \"Country Code\", \"Location City\",\n",
    "    \"Incident Year\", \"Incident Month\", \"Incident Day\",\n",
    "    \"Intentional Harm\", \"Autonomy Level\",\n",
    "    \"AI Task\", \"AI Methods\", \"AI System Description\", \"Data Inputs\",\n",
    "    \"Involving Minor\", \"Protected Characteristic\",\n",
    "    \"AI System\", \"Detrimental Content\", \"Impact on Critical Services\",\n",
    "    \"Multiple AI Interaction\", \"Embedded\", \"Entities\",\n",
    "    \"Physical Objects\", \"Deployed\", \"Estimated Date\",\n",
    "    \"Estimated Harm Quantities\", \"Special Interest Intangible Harm\",\n",
    "]\n",
    "\n",
    "print(\"âœ…  Configuration loaded\")\n",
    "print(f\"    Snapshot dir  : {SNAPSHOT_DIR}\")\n",
    "print(f\"    Output file   : {OUTPUT_PATH}\")\n",
    "print(f\"    Column mappings: incidents={len(INCIDENTS_COLUMNS)}  MIT={len(MIT_COLUMNS)}  \"\n",
    "      f\"GMF={len(GMF_COLUMNS)}  CSET={len(CSET_COLUMNS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ“¦  CELL 2 â€” INSTALL & IMPORT  (no editing needed)                         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "for pkg in [\"beautifulsoup4\", \"lxml\", \"requests\", \"openpyxl\"]:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"], check=True)\n",
    "\n",
    "import ast, re, tarfile, collections\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# Google Colab download helper (no-op outside Colab)\n",
    "try:\n",
    "    from google.colab import files as colab_files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    colab_files = None\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"âœ…  Libraries ready  |  Running in {'Google Colab' if IN_COLAB else 'local Jupyter'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ“¥  CELL 3 â€” DOWNLOAD LATEST SNAPSHOT  (no editing needed)                 â•‘\n",
    "# â•‘  Automatically finds and downloads the most recent AIID data export.        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Scrape the snapshots page for all .tar.bz2 links\n",
    "print(f\"Fetching snapshot list from {SNAPSHOT_PAGE_URL} ...\")\n",
    "html  = requests.get(SNAPSHOT_PAGE_URL, timeout=60).text\n",
    "soup  = BeautifulSoup(html, \"lxml\")\n",
    "links = []\n",
    "for a in soup.select(\"a[href]\"):\n",
    "    href = a[\"href\"]\n",
    "    if SNAPSHOT_FILTER in href:\n",
    "        if href.startswith(\"/\"):\n",
    "            href = BASE_URL + href\n",
    "        links.append(href)\n",
    "\n",
    "if not links:\n",
    "    raise RuntimeError(\n",
    "        \"No snapshot links found on the page â€” the site structure may have changed. \"\n",
    "        f\"Check {SNAPSHOT_PAGE_URL} manually.\"\n",
    "    )\n",
    "\n",
    "# 2. Pick the latest by timestamp (backup-YYYYMMDDHHMMSS.tar.bz2)\n",
    "def _ts(url):\n",
    "    m = re.search(r\"backup-(\\d{14})\\.tar\\.bz2\", url)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "latest_url = sorted(links, key=_ts)[-1]\n",
    "print(f\"Latest snapshot  : {latest_url}\")\n",
    "\n",
    "# 3. Download with progress indication\n",
    "tar_path = SNAPSHOT_DIR / latest_url.split(\"/\")[-1]\n",
    "if tar_path.exists():\n",
    "    print(f\"Already downloaded: {tar_path.name} â€” skipping download\")\n",
    "else:\n",
    "    print(\"Downloading ...\", end=\" \", flush=True)\n",
    "    with requests.get(latest_url, stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        total  = int(r.headers.get(\"content-length\", 0))\n",
    "        done   = 0\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 512):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    done += len(chunk)\n",
    "                    if total:\n",
    "                        print(f\"\\r  {done/1024/1024:.1f} MB / {total/1024/1024:.1f} MB\",\n",
    "                              end=\"\", flush=True)\n",
    "    print(f\"\\nâœ…  Downloaded: {tar_path.name}\")\n",
    "\n",
    "# 4. Extract\n",
    "print(\"Extracting ...\", end=\" \", flush=True)\n",
    "with tarfile.open(tar_path, \"r:bz2\") as tar:\n",
    "    tar.extractall(SNAPSHOT_DIR, filter=\"data\")  # 'data' filter suppresses Python 3.14 warning\n",
    "print(\"done\")\n",
    "\n",
    "# 5. Locate the five CSVs and store paths\n",
    "def _find(pattern):\n",
    "    matches = list(SNAPSHOT_DIR.rglob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find '{pattern}' under {SNAPSHOT_DIR}. \"\n",
    "            \"The snapshot may have a different internal structure.\"\n",
    "        )\n",
    "    return matches[0]\n",
    "\n",
    "INCIDENTS_PATH  = _find(\"incidents.csv\")\n",
    "MIT_PATH        = _find(\"classifications_MIT.csv\")\n",
    "GMF_PATH        = _find(\"classifications_GMF.csv\")\n",
    "CSET_PATH       = _find(\"classifications_CSETv1.csv\")\n",
    "DUPLICATES_PATH = _find(\"duplicates.csv\")\n",
    "\n",
    "print(\"\\nLocated files:\")\n",
    "for label, p in [(\"incidents\", INCIDENTS_PATH), (\"MIT\", MIT_PATH),\n",
    "                  (\"GMF\", GMF_PATH), (\"CSET\", CSET_PATH), (\"duplicates\", DUPLICATES_PATH)]:\n",
    "    print(f\"  {label:<12} â†’ {p.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_schema_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ”  CELL 4 â€” SCHEMA HEALTH CHECK  (no editing needed)                      â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  Compares every column in your Cell 1 mappings against the actual files.    â•‘\n",
    "# â•‘  If a source file has been updated with renamed columns, this cell shows    â•‘\n",
    "# â•‘  you exactly which ones changed so you can fix Cell 1 and re-run.           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Load just the headers of each file (fast â€” no data read)\n",
    "_raw_incidents  = pd.read_csv(INCIDENTS_PATH,  nrows=0)\n",
    "_raw_mit        = pd.read_csv(MIT_PATH,        nrows=0)\n",
    "_raw_gmf        = pd.read_csv(GMF_PATH,        nrows=0)\n",
    "_raw_cset       = pd.read_csv(CSET_PATH,       nrows=0)\n",
    "_raw_duplicates = pd.read_csv(DUPLICATES_PATH, nrows=0)\n",
    "\n",
    "file_cols = {\n",
    "    \"incidents\"  : set(_raw_incidents.columns),\n",
    "    \"MIT\"        : set(_raw_mit.columns),\n",
    "    \"GMF\"        : set(_raw_gmf.columns),\n",
    "    \"CSETv1\"     : set(_raw_cset.columns),\n",
    "    \"duplicates\" : set(_raw_duplicates.columns),\n",
    "}\n",
    "\n",
    "check_maps = {\n",
    "    \"incidents\" : INCIDENTS_COLUMNS,\n",
    "    \"MIT\"       : MIT_COLUMNS,\n",
    "    \"GMF\"       : GMF_COLUMNS,\n",
    "    \"CSETv1\"    : CSET_COLUMNS,\n",
    "    \"duplicates\": {DUPLICATES_ID_COLUMN: DUPLICATES_ID_COLUMN},\n",
    "}\n",
    "\n",
    "all_ok    = True\n",
    "issues    = []\n",
    "new_cols  = []\n",
    "\n",
    "SEP = \"â”€\" * 68\n",
    "print(SEP)\n",
    "print(\"  ğŸ”  SCHEMA HEALTH CHECK\")\n",
    "print(SEP)\n",
    "\n",
    "for source, mapping in check_maps.items():\n",
    "    actual_cols   = file_cols[source]\n",
    "    expected_keys = set(mapping.keys())\n",
    "    missing       = expected_keys - actual_cols      # in config but not in file\n",
    "    added         = actual_cols   - expected_keys    # in file but not in config\n",
    "\n",
    "    # Filter noise from added (admin/notes columns we intentionally skip)\n",
    "    _noise = {\"Namespace\", \"Published\", \"Incident Number\", \"_id\",\n",
    "               \"Annotator\", \"Peer Reviewer\", \"Annotation Status\", \"Quality Control\"}\n",
    "    added_meaningful = added - _noise - {c for c in added if\n",
    "                                          \"Snippet\" in c or \"Discussion\" in c\n",
    "                                          or \"Notes\" in c or \"Potential\" in c}\n",
    "\n",
    "    status = \"âœ…\" if not missing else \"âŒ\"\n",
    "    print(f\"\\n  {status}  {source}  ({len(actual_cols)} columns in file)\")\n",
    "\n",
    "    if missing:\n",
    "        all_ok = False\n",
    "        for col in sorted(missing):\n",
    "            issues.append((source, col, mapping[col]))\n",
    "            # Suggest close matches\n",
    "            close = [c for c in actual_cols\n",
    "                     if col.lower().replace(\" \", \"\") in c.lower().replace(\" \", \"\")\n",
    "                     or c.lower().replace(\" \", \"\") in col.lower().replace(\" \", \"\")]\n",
    "            hint  = f\"  â†’ Maybe renamed to: {close}\" if close else \"\"\n",
    "            print(f\"      âŒ  MISSING: '{col}' (maps to master '{mapping[col]}'){hint}\")\n",
    "\n",
    "    if added_meaningful:\n",
    "        for col in sorted(added_meaningful):\n",
    "            new_cols.append((source, col))\n",
    "            print(f\"      â„¹ï¸   NEW COLUMN not yet mapped: '{col}'\")\n",
    "\n",
    "print(f\"\\n{SEP}\")\n",
    "if all_ok:\n",
    "    print(\"  âœ…  All column mappings match â€” safe to continue\")\n",
    "else:\n",
    "    print(f\"  âŒ  {len(issues)} column(s) not found in source files\")\n",
    "    print(\"\")\n",
    "    print(\"  HOW TO FIX:\")\n",
    "    print(\"  1. Look at the MISSING entries above and note the new column name\")\n",
    "    print(\"  2. Go back to Cell 1 (âš™ï¸ Configuration)\")\n",
    "    print(\"  3. Find the old name in the relevant mapping dict and update it\")\n",
    "    print(\"  4. Re-run from Cell 1 onwards\")\n",
    "if new_cols:\n",
    "    print(f\"\\n  â„¹ï¸   {len(new_cols)} new column(s) exist in source files but aren't mapped.\")\n",
    "    print(\"  To include them, add entries to the relevant dict in Cell 1.\")\n",
    "print(SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ“‚  CELL 5 â€” LOAD RAW DATA  (no editing needed)                            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "incidents_raw  = pd.read_csv(INCIDENTS_PATH,  low_memory=False)\n",
    "mit_raw        = pd.read_csv(MIT_PATH,        low_memory=False)\n",
    "gmf_raw        = pd.read_csv(GMF_PATH,        low_memory=False)\n",
    "cset_raw       = pd.read_csv(CSET_PATH,       low_memory=False)\n",
    "duplicates_raw = pd.read_csv(DUPLICATES_PATH)\n",
    "\n",
    "print(\"Raw file dimensions:\")\n",
    "print(f\"  incidents.csv           {incidents_raw.shape[0]:>5} rows  {incidents_raw.shape[1]:>3} cols\")\n",
    "print(f\"  classifications_MIT     {mit_raw.shape[0]:>5} rows  {mit_raw.shape[1]:>3} cols\")\n",
    "print(f\"  classifications_GMF     {gmf_raw.shape[0]:>5} rows  {gmf_raw.shape[1]:>3} cols\")\n",
    "print(f\"  classifications_CSETv1  {cset_raw.shape[0]:>5} rows  {cset_raw.shape[1]:>3} cols\")\n",
    "print(f\"  duplicates              {duplicates_raw.shape[0]:>5} rows\")\n",
    "\n",
    "# IDs to remove (duplicates list)\n",
    "dup_ids = set(duplicates_raw[DUPLICATES_ID_COLUMN].dropna().astype(int).tolist())\n",
    "print(f\"\\n  Duplicate IDs to remove : {len(dup_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_clean_incidents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ§¹  CELL 6 â€” CLEAN: incidents.csv  (no editing needed)                     â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  What this does:                                                             â•‘\n",
    "# â•‘  â€¢ Removes known duplicate incidents                                         â•‘\n",
    "# â•‘  â€¢ Drops MongoDB internal _id field                                          â•‘\n",
    "# â•‘  â€¢ Parses date â†’ datetime and extracts year                                  â•‘\n",
    "# â•‘  â€¢ Counts linked news reports per incident (report_count)                   â•‘\n",
    "# â•‘  â€¢ Converts JSON slug lists for deployer/developer/harmed to readable text  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def _parse_entity_list(val):\n",
    "    \"\"\"Convert JSON slug list e.g. ['youtube'] â†’ 'Youtube'\"\"\"\n",
    "    try:\n",
    "        items = ast.literal_eval(str(val))\n",
    "        return \", \".join(str(i).replace(\"-\", \" \").strip().title() for i in items)\n",
    "    except Exception:\n",
    "        return str(val).strip()\n",
    "\n",
    "def _count_reports(val):\n",
    "    try:\n",
    "        return len(ast.literal_eval(str(val)))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# Select only mapped columns (drop _id and anything unmapped)\n",
    "keep_inc = [c for c in INCIDENTS_COLUMNS if c in incidents_raw.columns]\n",
    "inc = incidents_raw[keep_inc].copy()\n",
    "inc = inc.rename(columns=INCIDENTS_COLUMNS)\n",
    "\n",
    "# Remove duplicates\n",
    "before = len(inc)\n",
    "inc = inc[~inc[\"Incident ID\"].isin(dup_ids)].copy()\n",
    "print(f\"Incidents after removing duplicates: {before} â†’ {len(inc)}\")\n",
    "\n",
    "# Parse date & year\n",
    "inc[\"date\"] = pd.to_datetime(inc[\"date\"], errors=\"coerce\")\n",
    "inc[\"year\"] = inc[\"date\"].dt.year.astype(\"Int64\")\n",
    "\n",
    "# Count reports\n",
    "inc[\"report_count\"] = inc[\"reports\"].apply(_count_reports)\n",
    "print(f\"Report count â€” min: {inc['report_count'].min()}  \"\n",
    "      f\"max: {inc['report_count'].max()}  mean: {inc['report_count'].mean():.1f}\")\n",
    "\n",
    "# Parse entity slugs â†’ readable\n",
    "inc[\"deployer\"]  = inc[\"deployer_raw\"].apply(_parse_entity_list)\n",
    "inc[\"developer\"] = inc[\"developer_raw\"].apply(_parse_entity_list)\n",
    "inc[\"harmed\"]    = inc[\"harmed_raw\"].apply(_parse_entity_list)\n",
    "\n",
    "# Drop raw working columns\n",
    "inc = inc.drop(columns=[\"reports\", \"deployer_raw\", \"developer_raw\", \"harmed_raw\"],\n",
    "               errors=\"ignore\")\n",
    "inc = inc.sort_values(\"Incident ID\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCleaned incidents: {inc.shape[0]} rows Ã— {inc.shape[1]} cols\")\n",
    "print(f\"Columns: {inc.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_clean_mit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ§¹  CELL 7 â€” CLEAN: classifications_MIT.csv  (no editing needed)           â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  What this does:                                                             â•‘\n",
    "# â•‘  â€¢ Keeps only columns defined in MIT_COLUMNS (Cell 1)                       â•‘\n",
    "# â•‘  â€¢ Strips numeric prefixes from domain labels                               â•‘\n",
    "# â•‘    e.g.  \"1. Discrimination and Toxicity\" â†’ \"Discrimination and Toxicity\"  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "keep_mit = [c for c in MIT_COLUMNS if c in mit_raw.columns]\n",
    "mit_c = mit_raw[keep_mit].copy()\n",
    "mit_c = mit_c.rename(columns=MIT_COLUMNS)\n",
    "mit_c = mit_c[~mit_c[\"Incident ID\"].isin(dup_ids)]\n",
    "\n",
    "# Strip numeric prefixes for readability\n",
    "if \"Risk Domain\" in mit_c.columns:\n",
    "    mit_c[\"Risk Domain\"]    = mit_c[\"Risk Domain\"].str.replace(r\"^\\d+\\.\\s*\", \"\", regex=True)\n",
    "if \"Risk Subdomain\" in mit_c.columns:\n",
    "    mit_c[\"Risk Subdomain\"] = mit_c[\"Risk Subdomain\"].str.replace(r\"^\\d+\\.\\d+\\.\\s*\", \"\", regex=True)\n",
    "\n",
    "mit_c = mit_c.sort_values(\"Incident ID\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Cleaned MIT: {mit_c.shape[0]} rows Ã— {mit_c.shape[1]} cols\")\n",
    "print(f\"Risk Domains ({mit_c['Risk Domain'].nunique()} unique):\")\n",
    "for domain, count in mit_c[\"Risk Domain\"].value_counts().items():\n",
    "    print(f\"  {count:4d}  {domain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_clean_gmf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ§¹  CELL 8 â€” CLEAN: classifications_GMF.csv  (no editing needed)           â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  What this does:                                                             â•‘\n",
    "# â•‘  â€¢ Keeps only the 3 confirmed classification columns (from GMF_COLUMNS)     â•‘\n",
    "# â•‘  â€¢ Drops Snippet, Discussion, and Potential columns (annotator working      â•‘\n",
    "# â•‘    material â€” not structured data for analysis)                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "keep_gmf = [c for c in GMF_COLUMNS if c in gmf_raw.columns]\n",
    "gmf_c = gmf_raw[keep_gmf].copy()\n",
    "gmf_c = gmf_c.rename(columns=GMF_COLUMNS)\n",
    "gmf_c = gmf_c[~gmf_c[\"Incident ID\"].isin(dup_ids)]\n",
    "gmf_c = gmf_c.sort_values(\"Incident ID\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Cleaned GMF: {gmf_c.shape[0]} rows Ã— {gmf_c.shape[1]} cols\")\n",
    "print(f\"Columns: {gmf_c.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nTop 10 Technical Failures:\")\n",
    "failures = []\n",
    "for v in gmf_c[\"Technical Failure\"].dropna():\n",
    "    failures.extend(f.strip() for f in str(v).split(\",\"))\n",
    "for fail, cnt in collections.Counter(failures).most_common(10):\n",
    "    print(f\"  {cnt:3d}  {fail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_clean_cset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ§¹  CELL 9 â€” CLEAN: classifications_CSETv1.csv  (no editing needed)        â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  What this does:                                                             â•‘\n",
    "# â•‘  â€¢ Keeps only columns defined in CSET_COLUMNS (Cell 1)                      â•‘\n",
    "# â•‘  â€¢ Drops admin, notes, and annotator-workflow columns automatically         â•‘\n",
    "# â•‘  â€¢ Normalises Location City blanks to null                                  â•‘\n",
    "# â•‘  â€¢ Normalises Incident Month from mixed formats (\"3\", \" 3\") â†’ \"March\"       â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "keep_cset = [c for c in CSET_COLUMNS if c in cset_raw.columns]\n",
    "cset_c = cset_raw[keep_cset].copy()\n",
    "cset_c = cset_c.rename(columns=CSET_COLUMNS)\n",
    "cset_c = cset_c[~cset_c[\"Incident ID\"].isin(dup_ids)]\n",
    "cset_c = cset_c.drop_duplicates(subset=\"Incident ID\")\n",
    "\n",
    "# Normalise Location City\n",
    "if \"Location City\" in cset_c.columns:\n",
    "    cset_c[\"Location City\"] = cset_c[\"Location City\"].str.strip().replace(\"\", pd.NA)\n",
    "\n",
    "# Normalise Incident Month: numeric strings â†’ full month names\n",
    "if \"Incident Month\" in cset_c.columns:\n",
    "    month_names = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "                   \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "    month_map   = {str(i): name for i, name in enumerate(month_names, 1)}\n",
    "    month_map.update({f\" {k}\": v for k, v in month_map.items()})  # handle leading space\n",
    "    cset_c[\"Incident Month\"] = cset_c[\"Incident Month\"].replace(month_map)\n",
    "\n",
    "cset_c = cset_c.sort_values(\"Incident ID\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Cleaned CSET: {cset_c.shape[0]} rows Ã— {cset_c.shape[1]} cols\")\n",
    "print(f\"Columns: {cset_c.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_build_master",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ”—  CELL 10 â€” BUILD MASTER DATASET  (no editing needed)                    â•‘\n",
    "# â•‘                                                                              â•‘\n",
    "# â•‘  Join strategy: incidents is the spine. All taxonomy tables join via        â•‘\n",
    "# â•‘  LEFT JOIN on Incident ID â€” every incident is preserved even if not         â•‘\n",
    "# â•‘  classified by all sources.                                                 â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# 1. Core identity columns\n",
    "inc_core = inc[[\"Incident ID\",\"date\",\"year\",\"title\",\"description\",\n",
    "                \"deployer\",\"developer\",\"harmed\",\"report_count\"]].copy()\n",
    "\n",
    "# 2. Data Sources provenance flag\n",
    "mit_ids  = set(mit_c[\"Incident ID\"])\n",
    "gmf_ids  = set(gmf_c[\"Incident ID\"])\n",
    "cset_ids = set(cset_c[\"Incident ID\"])\n",
    "\n",
    "def _sources(iid):\n",
    "    parts = []\n",
    "    if iid in mit_ids:  parts.append(\"MIT\")\n",
    "    if iid in gmf_ids:  parts.append(\"GMF\")\n",
    "    if iid in cset_ids: parts.append(\"CSETv1\")\n",
    "    return \" | \".join(parts) if parts else \"None\"\n",
    "\n",
    "inc_core[\"Data Sources\"] = inc_core[\"Incident ID\"].apply(_sources)\n",
    "\n",
    "# 3. Join MIT\n",
    "mit_join = mit_c[[\"Incident ID\",\"Risk Domain\",\"Risk Subdomain\",\n",
    "                   \"Responsible Entity\",\"Intent\",\"Timing\"]]\n",
    "master = inc_core.merge(mit_join, on=\"Incident ID\", how=\"left\")\n",
    "\n",
    "# 4. Join GMF\n",
    "gmf_join = gmf_c[[\"Incident ID\",\"AI Goal\",\"AI Technology\",\"Technical Failure\"]]\n",
    "master = master.merge(gmf_join, on=\"Incident ID\", how=\"left\")\n",
    "\n",
    "# 5. Join CSET (all cleaned columns)\n",
    "cset_join = [\"Incident ID\"] + [c for c in cset_c.columns if c != \"Incident ID\"]\n",
    "master = master.merge(cset_c[cset_join], on=\"Incident ID\", how=\"left\")\n",
    "\n",
    "master = master.sort_values(\"Incident ID\").reset_index(drop=True)\n",
    "\n",
    "# 6. Enforce column order from config (extra columns appended at end)\n",
    "ordered = [c for c in MASTER_COLUMN_ORDER if c in master.columns]\n",
    "extra   = [c for c in master.columns if c not in ordered]\n",
    "if extra:\n",
    "    print(f\"â„¹ï¸   Extra columns (appended at end): {extra}\")\n",
    "master = master[ordered + extra]\n",
    "\n",
    "print(f\"âœ…  Master dataset: {len(master):,} rows Ã— {len(master.columns)} columns\")\n",
    "print(f\"\\nCoverage breakdown:\")\n",
    "for src, cnt in master[\"Data Sources\"].value_counts().items():\n",
    "    pct = cnt / len(master) * 100\n",
    "    bar = \"â–ˆ\" * int(pct / 4)\n",
    "    print(f\"  {src:<28} {cnt:>5,}  ({pct:.1f}%)  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  âœ…  CELL 11 â€” VALIDATION  (no editing needed)                               â•‘\n",
    "# â•‘  Automated checks before export. Fix any âŒ errors before continuing.        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SEP = \"=\" * 55\n",
    "print(SEP)\n",
    "print(\"VALIDATION REPORT\")\n",
    "print(SEP)\n",
    "\n",
    "errors, warnings = [], []\n",
    "\n",
    "def _check(ok, msg_pass, msg_fail, is_warning=False):\n",
    "    if ok:\n",
    "        print(f\"  âœ…  {msg_pass}\")\n",
    "    else:\n",
    "        marker = \"âš ï¸ \" if is_warning else \"âŒ \"\n",
    "        print(f\"  {marker}  {msg_fail}\")\n",
    "        (warnings if is_warning else errors).append(msg_fail)\n",
    "\n",
    "# Row count\n",
    "_check(len(master) >= EXPECTED_MIN_INCIDENTS,\n",
    "       f\"Row count: {len(master):,} (â‰¥ {EXPECTED_MIN_INCIDENTS:,} expected)\",\n",
    "       f\"Row count {len(master):,} is below expected minimum {EXPECTED_MIN_INCIDENTS:,}\")\n",
    "\n",
    "# No duplicate Incident IDs\n",
    "dupes = master[\"Incident ID\"].duplicated().sum()\n",
    "_check(dupes == 0, \"No duplicate Incident IDs\", f\"{dupes} duplicate Incident IDs found\")\n",
    "\n",
    "# Core columns 100% filled\n",
    "core_cols = [\"Incident ID\",\"date\",\"year\",\"title\",\"description\",\n",
    "             \"deployer\",\"developer\",\"harmed\",\"report_count\",\"Data Sources\"]\n",
    "for col in core_cols:\n",
    "    nulls = master[col].isnull().sum()\n",
    "    _check(nulls == 0, f\"{col}: 100% filled\", f\"{col} has {nulls} nulls (should be 0)\")\n",
    "\n",
    "# MIT coverage\n",
    "mit_pct = master[\"Risk Domain\"].notna().mean() * 100\n",
    "_check(mit_pct >= EXPECTED_MIT_COVERAGE,\n",
    "       f\"MIT Risk Domain: {mit_pct:.1f}% filled\",\n",
    "       f\"MIT coverage {mit_pct:.1f}% is below expected {EXPECTED_MIT_COVERAGE}%\",\n",
    "       is_warning=True)\n",
    "\n",
    "# Year range\n",
    "yr_min, yr_max = master[\"year\"].min(), master[\"year\"].max()\n",
    "_check(yr_min >= 1980 and yr_max >= 2024,\n",
    "       f\"Year range: {yr_min} â†’ {yr_max}\",\n",
    "       f\"Unexpected year range: {yr_min} â†’ {yr_max}\",\n",
    "       is_warning=True)\n",
    "\n",
    "# No row explosion from joins\n",
    "_check(len(master) == len(inc_core),\n",
    "       f\"No row explosion from joins ({len(master):,} rows)\",\n",
    "       f\"Row explosion: master has {len(master):,} but incidents has {len(inc_core):,}\")\n",
    "\n",
    "# Lives lost sanity check\n",
    "max_lives = master[\"Lives Lost\"].max() if \"Lives Lost\" in master.columns else 0\n",
    "print(f\"  â„¹ï¸   Max Lives Lost: {max_lives}\")\n",
    "\n",
    "print(f\"\\n{SEP}\")\n",
    "if errors:\n",
    "    print(f\"  âŒ  {len(errors)} ERROR(S) â€” do not export until resolved:\")\n",
    "    for e in errors: print(f\"     â€¢ {e}\")\n",
    "else:\n",
    "    print(\"  âœ…  All checks passed â€” safe to export\")\n",
    "if warnings:\n",
    "    print(f\"  âš ï¸   {len(warnings)} WARNING(S) â€” review but export is still safe:\")\n",
    "    for w in warnings: print(f\"     â€¢ {w}\")\n",
    "print(SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ’¾  CELL 12 â€” EXPORT  (no editing needed)                                  â•‘\n",
    "# â•‘  Writes a 3-sheet Excel file and auto-downloads it in Google Colab.         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Style constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HDR_FILLS = {\n",
    "    \"identity\" : PatternFill(\"solid\", fgColor=\"1F3864\"),\n",
    "    \"coverage\" : PatternFill(\"solid\", fgColor=\"2E4057\"),\n",
    "    \"mit\"      : PatternFill(\"solid\", fgColor=\"1A6B3C\"),\n",
    "    \"gmf\"      : PatternFill(\"solid\", fgColor=\"7B3F00\"),\n",
    "    \"cset\"     : PatternFill(\"solid\", fgColor=\"4A235A\"),\n",
    "}\n",
    "GROUP_HEX  = {k: v.fgColor.rgb for k, v in HDR_FILLS.items()}\n",
    "HDR_FONT   = Font(name=\"Arial\", bold=True, color=\"FFFFFF\", size=9)\n",
    "BODY_FONT  = Font(name=\"Arial\", size=9)\n",
    "ALT_FILL   = PatternFill(\"solid\", fgColor=\"F5F5F5\")\n",
    "WHITE_FILL = PatternFill(\"solid\", fgColor=\"FFFFFF\")\n",
    "_s         = Side(style=\"thin\", color=\"D0D0D0\")\n",
    "BORDER     = Border(left=_s, right=_s, top=_s, bottom=_s)\n",
    "\n",
    "# Column â†’ colour group mapping (auto-derived from config)\n",
    "_identity_cols = [\"Incident ID\",\"date\",\"year\",\"title\",\"description\",\"deployer\",\"developer\",\"harmed\"]\n",
    "_coverage_cols = [\"Data Sources\",\"report_count\"]\n",
    "_mit_cols      = [\"Risk Domain\",\"Risk Subdomain\",\"Responsible Entity\",\"Intent\",\"Timing\"]\n",
    "_gmf_cols      = [\"AI Goal\",\"AI Technology\",\"Technical Failure\"]\n",
    "\n",
    "def _col_group(c):\n",
    "    if c in _identity_cols: return \"identity\"\n",
    "    if c in _coverage_cols: return \"coverage\"\n",
    "    if c in _mit_cols:      return \"mit\"\n",
    "    if c in _gmf_cols:      return \"gmf\"\n",
    "    return \"cset\"\n",
    "\n",
    "GROUP_LABELS = {\n",
    "    \"identity\" : \"â— INCIDENT IDENTITY  (incidents.csv â€” 100% complete)\",\n",
    "    \"coverage\" : \"â— COVERAGE  (derived)\",\n",
    "    \"mit\"      : \"â— RISK CLASSIFICATION â€” MIT  (~91% coverage)\",\n",
    "    \"gmf\"      : \"â— TECHNICAL ANALYSIS â€” GMF  (~24% coverage)\",\n",
    "    \"cset\"     : \"â— POLICY DETAIL â€” CSETv1  (~16% coverage)\",\n",
    "}\n",
    "\n",
    "\n",
    "def _write_master_sheet(wb, df):\n",
    "    ws   = wb.create_sheet(\"Master Dataset\")\n",
    "    cols = df.columns.tolist()\n",
    "    n    = len(cols)\n",
    "\n",
    "    # Row 1: title banner\n",
    "    ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=n)\n",
    "    c = ws.cell(row=1, column=1,\n",
    "                value=f\"AI Incident Database â€” Master Dataset  |  {len(df):,} incidents  |  {n} columns\")\n",
    "    c.font = Font(name=\"Arial\", bold=True, size=11, color=\"FFFFFF\")\n",
    "    c.fill = PatternFill(\"solid\", fgColor=\"0D1B2A\")\n",
    "    c.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    ws.row_dimensions[1].height = 24\n",
    "\n",
    "    # Row 2: colour-coded group banners\n",
    "    curr_grp, grp_start = None, 1\n",
    "    for ci, col in enumerate(cols, 1):\n",
    "        grp = _col_group(col)\n",
    "        if grp != curr_grp:\n",
    "            if curr_grp is not None:\n",
    "                ws.merge_cells(start_row=2, start_column=grp_start, end_row=2, end_column=ci-1)\n",
    "                cell = ws.cell(row=2, column=grp_start)\n",
    "                cell.value     = GROUP_LABELS.get(curr_grp, \"\")\n",
    "                cell.font      = Font(name=\"Arial\", bold=True, size=8, color=\"FFFFFF\")\n",
    "                cell.fill      = PatternFill(\"solid\", fgColor=GROUP_HEX[curr_grp])\n",
    "                cell.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "            curr_grp, grp_start = grp, ci\n",
    "    ws.merge_cells(start_row=2, start_column=grp_start, end_row=2, end_column=n)\n",
    "    cell = ws.cell(row=2, column=grp_start)\n",
    "    cell.value     = GROUP_LABELS.get(curr_grp, \"\")\n",
    "    cell.font      = Font(name=\"Arial\", bold=True, size=8, color=\"FFFFFF\")\n",
    "    cell.fill      = PatternFill(\"solid\", fgColor=GROUP_HEX[curr_grp])\n",
    "    cell.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    ws.row_dimensions[2].height = 18\n",
    "\n",
    "    # Row 3: column headers\n",
    "    for ci, col in enumerate(cols, 1):\n",
    "        cell = ws.cell(row=3, column=ci, value=col)\n",
    "        cell.font      = HDR_FONT\n",
    "        cell.fill      = HDR_FILLS[_col_group(col)]\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        cell.border    = BORDER\n",
    "    ws.row_dimensions[3].height = 36\n",
    "\n",
    "    # Data rows\n",
    "    for ri, row in enumerate(df.itertuples(index=False), start=4):\n",
    "        fill = ALT_FILL if ri % 2 == 0 else WHITE_FILL\n",
    "        for ci, val in enumerate(row, start=1):\n",
    "            if pd.isna(val):                        val = None\n",
    "            elif hasattr(val, \"strftime\"):          val = val.strftime(\"%Y-%m-%d\")\n",
    "            elif isinstance(val, float) and val == int(val): val = int(val)\n",
    "            cell = ws.cell(row=ri, column=ci, value=val)\n",
    "            cell.font      = BODY_FONT\n",
    "            cell.fill      = fill\n",
    "            cell.border    = BORDER\n",
    "            cell.alignment = Alignment(vertical=\"top\", wrap_text=False)\n",
    "\n",
    "    # Column widths\n",
    "    widths = {\n",
    "        \"Incident ID\":11,\"date\":13,\"year\":7,\"title\":40,\"description\":50,\n",
    "        \"deployer\":25,\"developer\":25,\"harmed\":25,\"Data Sources\":22,\"report_count\":10,\n",
    "        \"Risk Domain\":30,\"Risk Subdomain\":42,\"Responsible Entity\":16,\"Intent\":14,\"Timing\":16,\n",
    "        \"AI Goal\":35,\"AI Technology\":30,\"Technical Failure\":35,\n",
    "    }\n",
    "    for ci, col in enumerate(cols, 1):\n",
    "        ws.column_dimensions[get_column_letter(ci)].width = widths.get(col, 18)\n",
    "\n",
    "    ws.freeze_panes = ws.cell(row=4, column=3)\n",
    "    ws.auto_filter.ref = f\"A3:{get_column_letter(n)}3\"\n",
    "\n",
    "\n",
    "def _write_dict_sheet(wb, df):\n",
    "    ws = wb.create_sheet(\"Data Dictionary\")\n",
    "    ws.merge_cells(\"A1:E1\")\n",
    "    c = ws.cell(row=1, column=1, value=\"Data Dictionary â€” AI Incident Database Master Dataset\")\n",
    "    c.font = Font(name=\"Arial\", bold=True, size=12, color=\"FFFFFF\")\n",
    "    c.fill = PatternFill(\"solid\", fgColor=\"0D1B2A\")\n",
    "    c.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    ws.row_dimensions[1].height = 26\n",
    "\n",
    "    for ci, h in enumerate([\"Column\",\"Group\",\"Source\",\"Fill Rate\",\"Description\"], 1):\n",
    "        cell = ws.cell(row=2, column=ci, value=h)\n",
    "        cell.font      = HDR_FONT\n",
    "        cell.fill      = PatternFill(\"solid\", fgColor=\"1F3864\")\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        cell.border    = BORDER\n",
    "\n",
    "    # Auto-generate entries from actual master columns\n",
    "    descriptions = {\n",
    "        \"Incident ID\"       : \"Primary join key. Unique per incident.\",\n",
    "        \"date\"              : \"Date harm occurred (editor-resolved).\",\n",
    "        \"year\"              : \"Year derived from date â€” covers all incidents. Best for time-series.\",\n",
    "        \"title\"             : \"Short editor-written title of the incident.\",\n",
    "        \"description\"       : \"1â€“3 sentence neutral summary of what happened.\",\n",
    "        \"deployer\"          : \"Who deployed the AI. Cleaned from JSON slug format.\",\n",
    "        \"developer\"         : \"Who built the AI system.\",\n",
    "        \"harmed\"            : \"Who was harmed or nearly harmed.\",\n",
    "        \"Data Sources\"      : \"Which taxonomies classified this incident: MIT | GMF | CSETv1.\",\n",
    "        \"report_count\"      : \"Number of linked news articles. Proxy for media attention.\",\n",
    "        \"Risk Domain\"       : \"High-level risk category (7 values). Best for broad trend analysis.\",\n",
    "        \"Risk Subdomain\"    : \"Granular sub-category (21 values) nested under Risk Domain.\",\n",
    "        \"Responsible Entity\": \"Who caused the risk: AI / Human / Other.\",\n",
    "        \"Intent\"            : \"Intentional vs Unintentional vs Other.\",\n",
    "        \"Timing\"            : \"Pre-deployment vs Post-deployment.\",\n",
    "        \"AI Goal\"           : \"What the AI was trying to do (e.g. Autonomous Driving).\",\n",
    "        \"AI Technology\"     : \"ML/AI technique used (e.g. Object Detection).\",\n",
    "        \"Technical Failure\" : \"What technically failed (e.g. Generalization Failure).\",\n",
    "        \"Harm Domain\"       : \"Whether harm occurred in a recognised domain: yes / no / maybe.\",\n",
    "        \"Tangible Harm\"     : \"Level of tangible harm: definitively occurred / near miss / none.\",\n",
    "        \"AI Harm Level\"     : \"AI's contribution to harm severity.\",\n",
    "        \"Rights Violation\"  : \"Whether a legal or human rights violation occurred.\",\n",
    "        \"Lives Lost\"        : \"Fatality count.\",\n",
    "        \"Injuries\"          : \"Injury count.\",\n",
    "        \"Sector of Deployment\": \"Industry sector (ISIC classification).\",\n",
    "        \"Location Region\"   : \"World region: North America / Europe / Asia / Global.\",\n",
    "        \"Country Code\"      : \"ISO 2-letter country code.\",\n",
    "        \"Intentional Harm\"  : \"Whether harm was intentionally designed into the system.\",\n",
    "        \"Autonomy Level\"    : \"Autonomy1 (human-in-loop) through Autonomy4 (fully autonomous).\",\n",
    "    }\n",
    "\n",
    "    grp_colors = {\"Identity\":\"1F3864\",\"Coverage\":\"2E4057\",\"MIT\":\"1A6B3C\",\"GMF\":\"7B3F00\",\"CSETv1\":\"4A235A\"}\n",
    "    src_map    = {\"identity\":\"incidents.csv\",\"coverage\":\"Derived\",\n",
    "                  \"mit\":\"MIT\",\"gmf\":\"GMF\",\"cset\":\"CSETv1\"}\n",
    "\n",
    "    for ri, col in enumerate(df.columns, start=3):\n",
    "        grp  = _col_group(col)\n",
    "        grp_label = grp.upper() if grp not in (\"identity\",\"coverage\") else grp.capitalize()\n",
    "        fill_pct = f\"{df[col].notna().mean()*100:.0f}%\"\n",
    "        desc     = descriptions.get(col, \"â€”\")\n",
    "        bg       = ALT_FILL if ri % 2 == 0 else WHITE_FILL\n",
    "        for ci, val in enumerate([col, grp_label, src_map.get(grp,\"â€”\"), fill_pct, desc], 1):\n",
    "            cell = ws.cell(row=ri, column=ci, value=val)\n",
    "            cell.fill      = bg\n",
    "            cell.border    = BORDER\n",
    "            cell.alignment = Alignment(vertical=\"top\", wrap_text=True)\n",
    "            cell.font = (Font(name=\"Arial\", size=9, bold=True, color=grp_colors.get(grp_label, \"000000\"))\n",
    "                         if ci == 2 else BODY_FONT)\n",
    "\n",
    "    for col_letter, w in zip([\"A\",\"B\",\"C\",\"D\",\"E\"], [28,12,14,10,72]):\n",
    "        ws.column_dimensions[col_letter].width = w\n",
    "    ws.freeze_panes = \"A3\"\n",
    "\n",
    "\n",
    "def _write_coverage_sheet(wb, df):\n",
    "    ws = wb.create_sheet(\"Coverage Map\")\n",
    "    ws.merge_cells(\"A1:D1\")\n",
    "    c = ws.cell(row=1, column=1, value=\"Coverage Map â€” What you can analyse at each taxonomy level\")\n",
    "    c.font = Font(name=\"Arial\", bold=True, size=11, color=\"FFFFFF\")\n",
    "    c.fill = PatternFill(\"solid\", fgColor=\"0D1B2A\")\n",
    "    c.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    ws.row_dimensions[1].height = 24\n",
    "\n",
    "    for ci, h in enumerate([\"Data Sources\",\"Incidents\",\"% of Total\",\"What you can analyse\"], 1):\n",
    "        cell = ws.cell(row=2, column=ci, value=h)\n",
    "        cell.font = HDR_FONT\n",
    "        cell.fill = PatternFill(\"solid\", fgColor=\"1F3864\")\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        cell.border = BORDER\n",
    "\n",
    "    analysis_map = {\n",
    "        \"MIT\"                 : \"Risk domain trends, intent, timing, entity â€” the broadest lens\",\n",
    "        \"MIT | GMF\"           : \"All MIT analysis + technical failures, AI goals, technology types\",\n",
    "        \"MIT | GMF | CSETv1\"  : \"Full picture: risk + technical + policy + sector + geography + harm\",\n",
    "        \"MIT | CSETv1\"        : \"Risk + policy: sector, lives lost, location, harm level, rights\",\n",
    "        \"None\"                : \"Title, description, deployer, developer, harmed, report count only\",\n",
    "    }\n",
    "\n",
    "    total     = len(df)\n",
    "    breakdown = df[\"Data Sources\"].value_counts().reset_index()\n",
    "    breakdown.columns = [\"Data Sources\", \"Count\"]\n",
    "\n",
    "    for ri, row in enumerate(breakdown.itertuples(index=False), start=3):\n",
    "        bg  = ALT_FILL if ri % 2 == 0 else WHITE_FILL\n",
    "        pct = f\"{row.Count/total*100:.1f}%\"\n",
    "        for ci, val in enumerate([row._0, row.Count, pct, analysis_map.get(row._0, \"â€”\")], 1):\n",
    "            cell = ws.cell(row=ri, column=ci, value=val)\n",
    "            cell.font = BODY_FONT; cell.fill = bg\n",
    "            cell.border = BORDER\n",
    "            cell.alignment = Alignment(vertical=\"top\", wrap_text=True)\n",
    "\n",
    "    for ci, val in enumerate([\"TOTAL\", total, \"100%\", \"\"], 1):\n",
    "        cell = ws.cell(row=len(breakdown)+3, column=ci, value=val)\n",
    "        cell.font   = Font(name=\"Arial\", bold=True, size=9)\n",
    "        cell.fill   = PatternFill(\"solid\", fgColor=\"D9E2F3\")\n",
    "        cell.border = BORDER\n",
    "\n",
    "    for col_letter, w in zip([\"A\",\"B\",\"C\",\"D\"], [24,14,12,68]):\n",
    "        ws.column_dimensions[col_letter].width = w\n",
    "\n",
    "\n",
    "# Write and save\n",
    "wb = Workbook()\n",
    "wb.remove(wb.active)\n",
    "_write_master_sheet(wb, master)\n",
    "_write_dict_sheet(wb, master)\n",
    "_write_coverage_sheet(wb, master)\n",
    "wb.save(OUTPUT_PATH)\n",
    "\n",
    "print(f\"âœ…  Saved: {OUTPUT_PATH}\")\n",
    "print(f\"   Sheets : {[s.title for s in wb.worksheets]}\")\n",
    "print(f\"   Master : {len(master):,} rows Ã— {len(master.columns)} columns\")\n",
    "\n",
    "# Auto-download in Google Colab\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ“¥  Starting download to your computer...\")\n",
    "    colab_files.download(str(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ“Š  CELL 13 â€” SUMMARY STATISTICS  (no editing needed)                      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SEP = \"=\" * 55\n",
    "print(SEP)\n",
    "print(\"MASTER DATASET â€” FINAL SUMMARY\")\n",
    "print(SEP)\n",
    "\n",
    "print(f\"\\n  Total incidents  : {len(master):>6,}\")\n",
    "print(f\"  Total columns    : {len(master.columns):>6}\")\n",
    "print(f\"  Date range       : {int(master['year'].min())} â†’ {int(master['year'].max())}\")\n",
    "print(f\"  Output file      : {OUTPUT_PATH}\")\n",
    "\n",
    "print(f\"\\n  Coverage by taxonomy:\")\n",
    "for src, cnt in master[\"Data Sources\"].value_counts().items():\n",
    "    bar = \"â–ˆ\" * int(cnt / 30)\n",
    "    print(f\"    {src:<28}  {cnt:>5,}  ({cnt/len(master)*100:.1f}%)  {bar}\")\n",
    "\n",
    "print(f\"\\n  MIT Risk Domains:\")\n",
    "for domain, cnt in master[\"Risk Domain\"].value_counts().items():\n",
    "    bar = \"â–ˆ\" * int(cnt / 30)\n",
    "    print(f\"    {bar:<15} {cnt:>4}  {domain}\")\n",
    "\n",
    "print(f\"\\n  Incidents by year (2015 onwards):\")\n",
    "year_counts = master[master[\"year\"] >= 2015][\"year\"].value_counts().sort_index()\n",
    "for yr, cnt in year_counts.items():\n",
    "    bar = \"â–ˆ\" * int(cnt / 10)\n",
    "    print(f\"    {int(yr)}  {bar:<40} {cnt}\")\n",
    "\n",
    "print(f\"\\n  Top 10 deployers:\")\n",
    "all_deployers = []\n",
    "for v in master[\"deployer\"].dropna():\n",
    "    all_deployers.extend(d.strip().lower() for d in str(v).split(\",\"))\n",
    "for entity, cnt in collections.Counter(all_deployers).most_common(10):\n",
    "    print(f\"    {cnt:>4}  {entity}\")\n",
    "\n",
    "print(f\"\\n{SEP}\")\n",
    "print(f\"  âœ…  Master dataset ready for analysis\")\n",
    "print(SEP)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
